{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "from implicit.als import AlternatingLeastSquares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# Train, valid, and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we get preprocessed train, valid, test data from the Amazon beauty dataset. In this implementation, we care only about 5 core leave one out case. Refer to https://amazon-reviews-2023.github.io/data_processing/5core.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"5core_last_out_w_his_All_Beauty\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['user_id', 'parent_asin', 'rating', 'timestamp', 'history'],\n",
       "        num_rows: 2029\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['user_id', 'parent_asin', 'rating', 'timestamp', 'history'],\n",
       "        num_rows: 253\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['user_id', 'parent_asin', 'rating', 'timestamp', 'history'],\n",
       "        num_rows: 253\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_All_Beauty\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rating': 5.0,\n",
       " 'title': 'Such a lovely scent but not overpowering.',\n",
       " 'text': \"This spray is really nice. It smells really good, goes on really fine, and does the trick. I will say it feels like you need a lot of it though to get the texture I want. I have a lot of hair, medium thickness. I am comparing to other brands with yucky chemicals so I'm gonna stick with this. Try it!\",\n",
       " 'images': [],\n",
       " 'asin': 'B00YQ6X8EO',\n",
       " 'parent_asin': 'B00YQ6X8EO',\n",
       " 'user_id': 'AGKHLEW2SOWHNMFQIJGBECAF7INQ',\n",
       " 'timestamp': 1588687728923,\n",
       " 'helpful_vote': 0,\n",
       " 'verified_purchase': True}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['full'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Ratings matrix from train and valid dataset\n",
    "\n",
    "In this project, we are not going to \"train\" on anything. So we'll just use train and valid dataset to construct all ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = []\n",
    "for i in range(len(dataset['train'])):\n",
    "    ratings.append({\n",
    "        'user_id': dataset['train'][i]['user_id'],\n",
    "        'parent_asin': dataset['train'][i]['parent_asin'],\n",
    "        'rating': float(dataset['train'][i]['rating'])\n",
    "    })\n",
    "\n",
    "for j in range(len(dataset['valid'])):\n",
    "    ratings.append({\n",
    "        'user_id': dataset['valid'][j]['user_id'],\n",
    "        'parent_asin': dataset['valid'][j]['parent_asin'],\n",
    "        'rating': float(dataset['valid'][j]['rating'])\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.DataFrame(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert user and product IDs to integer indices for the matrix\n",
    "user_mapping = {user: idx for idx, user in enumerate(ratings_df['user_id'].unique())}\n",
    "product_mapping = {product: idx for idx, product in enumerate(ratings_df['parent_asin'].unique())}\n",
    "reverse_user_mapping = {idx: user for user, idx in user_mapping.items()}\n",
    "reverse_product_mapping = {idx: product for product, idx in product_mapping.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to scipy sparse matrix\n",
    "ratings_df['user_id'] = ratings_df['user_id'].map(user_mapping)\n",
    "ratings_df['parent_asin'] = ratings_df['parent_asin'].map(product_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sparse matrix\n",
    "user_item = coo_matrix(\n",
    "    (ratings_df['rating'], (ratings_df['user_id'], ratings_df['parent_asin']))\n",
    ").tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 1224.32it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the ALS model\n",
    "model = AlternatingLeastSquares(factors=20, regularization=0.1, iterations=50)\n",
    "\n",
    "# Train the model\n",
    "model.fit(user_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = np.arange(user_item.shape[0])\n",
    "item_ids, scores = model.recommend(user_ids, user_item[user_ids], N=30)\n",
    "\n",
    "def map_value(x):\n",
    "    return reverse_product_mapping.get(x)  \n",
    "\n",
    "vectorized_map = np.vectorize(map_value)\n",
    "recommended_products = vectorized_map(item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(253, 30)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = [x['parent_asin'] for x in dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B0BTJ6SYKB\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(ground_truth):\n",
    "    if x in recommended_products[i]:\n",
    "        print(x)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendations(recommendations, ground_truth):\n",
    "    \"\"\"\n",
    "    Evaluate Hit Rate and Recall for the recommendations.\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    recall_sum = 0\n",
    "    total_relevant_items = 0\n",
    "\n",
    "    for i, actual_items in enumerate(ground_truth):\n",
    "        recommended_items = recommendations[i]\n",
    "        actual_set = set([actual_items])\n",
    "\n",
    "        # Calculate hits (whether there's any overlap)\n",
    "        if any(item in actual_set for item in recommended_items):\n",
    "            hits += 1\n",
    "\n",
    "        # Calculate recall (proportion of relevant items in recommendations)\n",
    "        relevant_items = actual_set.intersection(recommended_items)\n",
    "        recall_sum += len(relevant_items)\n",
    "        total_relevant_items += len(actual_set)\n",
    "\n",
    "    # Hit Rate: Proportion of users with at least one hit\n",
    "    hit_rate = hits / len(ground_truth)\n",
    "\n",
    "    # Recall: Average recall across all users\n",
    "    recall = recall_sum / total_relevant_items\n",
    "\n",
    "    return {'hit_rate': hit_rate, 'recall': recall}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit rate is 0.1067193675889328\n",
      "recall is 0.1067193675889328\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_recommendations(recommended_products, ground_truth)\n",
    "print('hit rate is', results['hit_rate'])\n",
    "print('recall is', results['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cf_retrive.npy', 'wb') as f:\n",
    "    np.save(f, recommended_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs294",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
